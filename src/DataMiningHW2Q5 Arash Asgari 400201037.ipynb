{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# installation"
      ],
      "metadata": {
        "id": "e2_vFl26Kh9i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tP82gRUdIb2",
        "outputId": "fd28ae28-a871-4daa-ca3d-4bd491e1a557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 35 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 48.1 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa1PvehvdXWq"
      },
      "outputs": [],
      "source": [
        "! apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "! wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "! tar xf spark-3.2.1-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmdGphNedaNQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffyV-7EGdevo"
      },
      "outputs": [],
      "source": [
        "! pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EqK6ol7ddihP",
        "outputId": "9c0ac15a-8bd2-4c7b-dbf8-449060cccc12"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.2.1-bin-hadoop3.2'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYEyB3PNdz3f"
      },
      "outputs": [],
      "source": [
        "# First Method\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hLyt1s9bdsR-",
        "outputId": "3388c95f-f748-4f94-d996-ce39f3b2b997"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.2.1'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark.version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5"
      ],
      "metadata": {
        "id": "_NOb7lYrKnLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Dataset"
      ],
      "metadata": {
        "id": "Hc_3_HXLK0id"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIJiLo0w3TAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41dce4cb-7530-41df-be54-7efb5704db64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-07 14:48:09--  https://raw.githubusercontent.com/optimopium/PySpark-walkthrough/main/data/Marvel-graph.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1666954 (1.6M) [text/plain]\n",
            "Saving to: ‘Marvel-graph.txt.1’\n",
            "\n",
            "Marvel-graph.txt.1  100%[===================>]   1.59M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-04-07 14:48:09 (80.9 MB/s) - ‘Marvel-graph.txt.1’ saved [1666954/1666954]\n",
            "\n",
            "--2022-04-07 14:48:09--  http://marvel/\n",
            "Resolving marvel (marvel)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘marvel’\n",
            "FINISHED --2022-04-07 14:48:09--\n",
            "Total wall clock time: 0.2s\n",
            "Downloaded: 1 files, 1.6M in 0.02s (80.9 MB/s)\n",
            "--2022-04-07 14:48:09--  https://raw.githubusercontent.com/optimopium/PySpark-walkthrough/main/data/Marvel-names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 332420 (325K) [text/plain]\n",
            "Saving to: ‘Marvel-names.txt’\n",
            "\n",
            "Marvel-names.txt    100%[===================>] 324.63K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-04-07 14:48:09 (25.6 MB/s) - ‘Marvel-names.txt’ saved [332420/332420]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://raw.githubusercontent.com/optimopium/PySpark-walkthrough/main/data/Marvel-graph.txt\" \"marvel\"\n",
        "!wget \"https://raw.githubusercontent.com/optimopium/PySpark-walkthrough/main/data/Marvel-names.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Spark(Parallel BFS)\n",
        "\n",
        "The Time complexity of BFS is time of 0(β log n) when 0(n 1+1/β) processors are used.  \n"
      ],
      "metadata": {
        "id": "emUnxw3tLHiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, We divide the BFS into Three parts: \n",
        "1.   Initialization\n",
        "> In this stage, we set all nodes' status to \"Not ready\", except the start node which is set \"ready\". The distance for all nodes is infinity at the start except the starting node which is 0.\n",
        "\n",
        "\n",
        "2.   Map\n",
        "> Here we take all the nodes with \"ready\" as their status and explore their connections. We turn the status of each node connected into ready and return them in our results list with their id as key. If one of the nodes that is connected to our ready node is the target one, we add to accumulator to save the number of nodes connected to the target on that level. At last, we return the read node we had found with the status of Searched.\n",
        "\n",
        "\n",
        "3.   Reduce\n",
        ">In Reduce we only keep the most advanced and minium distance of each node and return them.\n"
      ],
      "metadata": {
        "id": "3ITv5s5zLcMv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH6iOtfHn6B6"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZqw6OVB6Y_K"
      },
      "outputs": [],
      "source": [
        "def mapNode(node):\n",
        "    target = node[0]\n",
        "    data = node[1]\n",
        "    connections = data[0]\n",
        "    distance = data[1]\n",
        "    searchStatus = data[2]\n",
        "    results = []\n",
        "    if (searchStatus == 'READY'):\n",
        "        for connection in connections:\n",
        "            newTarget = connection\n",
        "            newDistance = distance + 1\n",
        "            newStatus = 'READY'\n",
        "            if (targetB == connection):\n",
        "                counter.add(1)\n",
        "            newEntry = (newTarget, ([], newDistance, newStatus))\n",
        "            results.append(newEntry)\n",
        "        searchStatus = 'SEARCHED'\n",
        "    \n",
        "    results.append((target, (connections, distance, searchStatus)))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3RJ1kfl7BlV"
      },
      "outputs": [],
      "source": [
        "def reduceNode(data1, data2):\n",
        "    connections1 = data1[0]\n",
        "    connections2 = data2[0]\n",
        "    distance1 = data1[1]\n",
        "    distance2 = data2[1]\n",
        "    searchStatus1 = data1[2]\n",
        "    searchStatus2 = data2[2]\n",
        "    distance = 10000\n",
        "    searchStatus = 'UNSEARCHED'\n",
        "    connections = []\n",
        "    if (len(connections) > 0):\n",
        "        connections.extend(connections1)\n",
        "    if (len(connections2) > 0):\n",
        "        connections.extend(connections2)\n",
        "    # Preserve minimum distance\n",
        "    if (distance1 < distance):\n",
        "        distance = distance1\n",
        "    if (distance2 < distance):\n",
        "        distance = distance2\n",
        "    # Preserve the most advanced searchStatus\n",
        "    if (searchStatus1 == 'NOT READY' and \n",
        "       (searchStatus2 == 'READY' or searchStatus2 == 'SEARCHED')):\n",
        "           searchStatus = searchStatus2\n",
        "    if (searchStatus1 == 'READY' and searchStatus2 == 'SEARCHED'):\n",
        "           searchStatus = searchStatus2\n",
        "    if (searchStatus2 == 'NOT READY' and \n",
        "       (searchStatus1 == 'READY' or searchStatus1 == 'SEARCHED')):\n",
        "           searchStatus = searchStatus1\n",
        "    if (searchStatus2 == 'READY' and searchStatus1 == 'SEARCHED'):\n",
        "           searchStatus = searchStatus1\n",
        "    \n",
        "    return (connections, distance, searchStatus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kycdm4gu-ncy"
      },
      "outputs": [],
      "source": [
        "def toNode(line):\n",
        "    data = line.split()\n",
        "    target = data[0]\n",
        "    connections = data[1:]\n",
        "    searchStatus = 'NOT READY'\n",
        "    distance = 10000\n",
        "    if (target == targetA):\n",
        "        searchStatus = 'READY'\n",
        "        distance = 0\n",
        "    return (target, (connections, distance, searchStatus))\n",
        "def transformInput(text_path):\n",
        "    input = sc.textFile(text_path)\n",
        "    return input.map(toNode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDQRMExf_HUQ",
        "outputId": "cdfd8b6b-4d3a-4dd3-f6e2-a23619bdd5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target B was found 4 levels(degree of separation) from Target A and was connected to 3 super heros at that level.\n"
          ]
        }
      ],
      "source": [
        "path_to_text = \"Marvel-graph.txt\"\n",
        "targetA = \"90\"\n",
        "targetB = \"98\"\n",
        "counter = sc.accumulator(0)\n",
        "iteratingRDD = transformInput(path_to_text)\n",
        "for iteration in range(0, 20):\n",
        "    mapped = iteratingRDD.flatMap(mapNode)\n",
        "    mapped.collect()\n",
        "    if (counter.value > 0):\n",
        "        print(\"Target B was found at \" + str(iteration+1) +  \n",
        "        \" levels(degree of separation) from Target A and was connected to \" +           \n",
        "        str(counter.value) + \" super heros at that level.\")\n",
        "        break\n",
        "    iteratingRDD = mapped.reduceByKey(reduceNode)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### most popular heros"
      ],
      "metadata": {
        "id": "q-C-OaHJhw4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType, IntegerType\n",
        "from pyspark.sql.functions import udf\n",
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "Uw8dcFZSbHJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_friends = udf(lambda z: len(z.split())-1, StringType())\n",
        "extract_node = udf(lambda z: z.split()[0], StringType())\n",
        "extract_name = udf(lambda z: z.split()[1], StringType())"
      ],
      "metadata": {
        "id": "t5riunYQbRs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "marvel_df = spark.read.text(\"Marvel-graph.txt\")\n",
        "marvel_df = marvel_df.withColumn(\"num_of_friends\", num_of_friends('value'))\n",
        "marvel_df = marvel_df.withColumn(\"source_node\", extract_node('value'))\n",
        "names_df = spark.read.text(\"Marvel-names.txt\")\n",
        "names_df = names_df.withColumn(\"source_node\", extract_node('value'))\n",
        "names_df = names_df.withColumn(\"name\", extract_name('value'))\n",
        "\n",
        "joined_marvel_df = marvel_df.join(names_df,names_df.source_node ==  marvel_df.source_node,\"inner\")"
      ],
      "metadata": {
        "id": "nK2Jgh25XY5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "popular = marvel_df.agg(F.max(\"num_of_friends\").alias(\"best\")).collect()\n",
        "most_friends = popular[0]['best']\n",
        "heros_with_most_friends = joined_marvel_df.filter(marvel_df[\"num_of_friends\"] == most_friends).collect()\n",
        "for hero in heros_with_most_friends:\n",
        "  print(hero[\"name\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCvPad6wc2nJ",
        "outputId": "90cc62bb-3c01-4448-ea62-5df0d100231f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"HUSSAR\"\n",
            "\"ERIC\n",
            "\"SCARFE,\n",
            "\"CARDINAL/CLEMDENON\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ordinary BFS\n",
        "\n",
        "The Time complexity of BFS is O(V + E) when Adjacency List is used"
      ],
      "metadata": {
        "id": "2RQGkvP4ZB5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = {\n",
        "  '5' : ['3','7'],\n",
        "  '3' : ['2', '4'],\n",
        "  '7' : ['8'],\n",
        "  '2' : [],\n",
        "  '4' : ['8'],\n",
        "  '8' : []\n",
        "}\n",
        "\n",
        "visited = [] # List for visited nodes.\n",
        "queue = []     #Initialize a queue\n",
        "\n",
        "def bfs(visited, graph, node): #function for BFS\n",
        "  visited.append(node)\n",
        "  queue.append(node)\n",
        "\n",
        "  while queue:          # Creating loop to visit each node\n",
        "    m = queue.pop(0) \n",
        "    print (m, end = \" \") \n",
        "\n",
        "    for neighbour in graph[m]:\n",
        "      if neighbour not in visited:\n",
        "        visited.append(neighbour)\n",
        "        queue.append(neighbour)\n",
        "\n",
        "# Driver Code\n",
        "print(\"Following is the Breadth-First Search\")\n",
        "bfs(visited, graph, '5')    # function calling"
      ],
      "metadata": {
        "id": "n6YamGkiZFag"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "DataMiningHW2Q5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "e2_vFl26Kh9i"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}